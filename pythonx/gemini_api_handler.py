# pythonx/gemini_api_handler.py
# Python backend for the Vim Gemini AI plugin.
# Handles all direct interactions with the Google Gemini API.

import google.generativeai as genai
import os
import json
import uuid # For generating unique session IDs
import logging # For potential debugging

# Configure logging (optional, for debugging Python side)
# logging.basicConfig(filename='/tmp/vim_gemini.log', level=logging.DEBUG,
#                     format='%(asctime)s - %(levelname)s - %(message)s')

# Dictionary to hold active chat sessions in memory.
# Key: session_id (str), Value: genai.GenerativeModel.start_chat() object
_active_chat_sessions = {}

def _configure_gemini(api_key_source):
    """
    Configures the Gemini API.
    Args:
        api_key_source (str): If starts with '/', treats as a file path.
                              Otherwise, treats as an environment variable name.
                              Also expands '~' in file paths.
    Raises ValueError if the API key is not found.
    """
    api_key = None
    if api_key_source.startswith('/') or api_key_source.startswith('~'):
        # Assume it's a file path, expand user home
        api_key_path = os.path.expanduser(api_key_source)
        try:
            with open(api_key_path, 'r') as f:
                api_key = f.read().strip()
            # logging.debug(f"Gemini API configured from file: {api_key_path}")
        except FileNotFoundError:
            raise ValueError(f"API key file not found at: {api_key_path}")
        except Exception as e:
            raise ValueError(f"Error reading API key from file {api_key_path}: {e}")
    else:
        # Assume it's an environment variable name
        api_key = os.getenv(api_key_source)
        # logging.debug(f"Gemini API configured from environment variable: {api_key_source}")

    if not api_key:
        raise ValueError(f"Gemini API key not found from source: '{api_key_source}'. Please check your config.")
    genai.configure(api_key=api_key)

def _extract_text_from_response(response):
    """
    Helper to safely extract text from a Gemini model response.
    """
    if response and response.candidates and response.candidates[0].content.parts:
        # Concatenate text from all parts in the first candidate
        return "\n".join([part.text for part in response.candidates[0].content.parts if hasattr(part, 'text')])
    return ""

# ============================================================================
# Single-Turn Content Generation
# ============================================================================

def generate_gemini_content(prompt_text, api_key_source, model_name="gemini-pro"):
    """
    Calls the Gemini API with the given prompt text for a single turn.

    Args:
        prompt_text (str): The text prompt to send to the Gemini model.
        api_key_source (str): The source for the API key (file path or env var name).
        model_name (str): The specific Gemini model to use (e.g., 'gemini-pro').

    Returns:
        str: A JSON string indicating success/failure and the generated text or error.
    """
    try:
        _configure_gemini(api_key_source) # Use the new _configure_gemini
        model = genai.GenerativeModel(model_name)
        # logging.debug(f"Sending single-turn prompt to {model_name}: {prompt_text[:100]}...")
        response = model.generate_content(prompt_text)
        
        generated_text = _extract_text_from_response(response)
        if generated_text:
            # logging.debug("Single-turn content generated successfully.")
            return json.dumps({"success": True, "text": generated_text})
        else:
            # logging.warning("Single-turn generation returned no text content.")
            return json.dumps({"success": False, "error": "No text content generated by Gemini."})
    except Exception as e:
        # logging.error(f"Error in generate_gemini_content: {e}", exc_info=True)
        return json.dumps({"success": False, "error": str(e)})

# ============================================================================
# Chat Session Management
# ============================================================================

def start_gemini_chat_session(api_key_source, initial_history=None):
    """
    Starts a new Gemini chat session and returns a unique session ID.

    Args:
        api_key_source (str): The source for the API key (file path or env var name).
        initial_history (list, optional): A list of dictionaries representing
                                         previous messages for a persistent chat.
                                         Each dict: {"role": "user/model", "parts": "text"}.

    Returns:
        str: A JSON string with {"success": bool, "session_id": str, "message": str, "error": str}.
    """
    try:
        _configure_gemini(api_key_source) # Use the new _configure_gemini
        model = genai.GenerativeModel('gemini-2.5-flash') # Chat usually uses a consistent model

        parsed_history = []
        if initial_history:
            for item in initial_history:
                # Ensure parts is a list of Part objects
                parts_list = []
                if isinstance(item.get('parts'), list):
                    for p in item['parts']:
                        parts_list.append(genai.types.Part(text=p))
                elif isinstance(item.get('parts'), str):
                    parts_list.append(genai.types.Part(text=item['parts']))
                else:
                    # Handle other potential types or log error
                    # logging.warning(f"Unexpected part format in history: {item.get('parts')}")
                    continue # Skip this malformed part

                parsed_history.append(genai.types.Content(role=item['role'], parts=parts_list))
        
        chat = model.start_chat(history=parsed_history)
        session_id = str(uuid.uuid4())
        _active_chat_sessions[session_id] = chat
        # logging.debug(f"New chat session started: {session_id}")
        return json.dumps({"success": True, "session_id": session_id, "message": "New chat session started."})
    except Exception as e:
        # logging.error(f"Error starting chat session: {e}", exc_info=True)
        return json.dumps({"success": False, "error": str(e)})

def send_gemini_chat_message(session_id, message_text, api_key_source):
    """
    Sends a message to an existing chat session and returns the model's response.

    Args:
        session_id (str): The ID of the active chat session.
        message_text (str): The user's message to send.
        api_key_source (str): The source for the API key (file path or env var name).

    Returns:
        str: A JSON string with {"success": bool, "text": str, "error": str}.
    """
    try:
        _configure_gemini(api_key_source) # Use the new _configure_gemini
        chat = _active_chat_sessions.get(session_id)
        if not chat:
            return json.dumps({"success": False, "error": f"Chat session '{session_id}' not found. It might have ended or Vim restarted."})

        # logging.debug(f"Sending message to chat {session_id}: {message_text[:100]}...")
        response = chat.send_message(message_text)
        
        generated_text = _extract_text_from_response(response)
        if generated_text:
            # logging.debug(f"Message sent and response received for session {session_id}.")
            return json.dumps({"success": True, "text": generated_text})
        else:
            # logging.warning(f"Chat message sent but no text content generated for session {session_id}.")
            return json.dumps({"success": False, "error": "No text content generated by Gemini for this message."})
    except Exception as e:
        # logging.error(f"Error sending chat message for session {session_id}: {e}", exc_info=True)
        return json.dumps({"success": False, "error": str(e)})

def get_gemini_chat_history(session_id):
    """
    Retrieves the full history of a given chat session.

    Args:
        session_id (str): The ID of the active chat session.

    Returns:
        str: A JSON string with {"success": bool, "history": list, "error": str}.
             History is a list of {"role": str, "parts": str} dictionaries.
    """
    chat = _active_chat_sessions.get(session_id)
    if not chat:
        return json.dumps({"success": False, "error": f"Chat session '{session_id}' not found."})
    
    history_list = []
    for content in chat.history:
        parts_text = []
        for part in content.parts:
            if hasattr(part, 'text'):
                parts_text.append(part.text)
        history_list.append({"role": content.role, "parts": "\n".join(parts_text)})
    
    # logging.debug(f"Retrieved history for session {session_id}")
    return json.dumps({"success": True, "history": history_list})

def end_gemini_chat_session(session_id):
    """
    Ends an active chat session and removes it from memory.

    Args:
        session_id (str): The ID of the chat session to end.

    Returns:
        str: A JSON string with {"success": bool, "message": str, "error": str}.
    """
    if session_id in _active_chat_sessions:
        del _active_chat_sessions[session_id]
        # logging.debug(f"Chat session {session_id} ended and removed.")
        return json.dumps({"success": True, "message": f"Session '{session_id}' ended."})
    else:
        return json.dumps({"success": False, "error": f"Chat session '{session_id}' not found."})
